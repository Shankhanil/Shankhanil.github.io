---
---

@string{aps = {American Physical Society,}}

@article{resensenet,
  bibtex_show={true},
  title={*reSenseNet: Ensemble Early Fusion Deep Learning Architecture for Multimodal Sentiment},
  author={Shankhanil Ghosh, Chhanda Saha, Nagamani Molakathala, Souvik Ghosh, Dhananjay Singh},
  year={2021},
  publisher={Springer LNCS},
  selected={true},
  pdf={reSenseNet__A_novel_method_for_multi_modal_sentiment_analysis_IHCI_CRC.pdf}
}


@article{speech-scis,
  bibtex_show={true},
  title={*Speech@SCIS : Annotated indian video dataset for cross-modal research},
  author={Shankhanil Ghosh, Chhanda Saha, Nagamani Molakathala, Souvik Ghosh, Rittam Das},
  year={2021},
  publisher={Springer Nature},
}

@article{neuragen,
  bibtex_show={true},
  title={*NeuraGen-A  Low-Resource Neural Network based approach for Gender Classification},
  author={Shankhanil Ghosh, Chhanda Saha, Nagamani Molakathala},
  year={2021},
  abstract={Human voice is the source of several important information. This is in the form of  features. These Features help in interpreting various features associated with the speaker and speech. The speaker dependent work researchers  are targeted towards speaker identification, Speaker verification, speaker biometric, forensics using feature, and cross-modal matching via speech and face images. In such context research, it is a very difficult task to come across clean, and well annotated publicly available speech corpus as data set. Acquiring volunteers to generate such dataset is also very expensive, not to mention the enormous amount of effort and time researchers spend to gather such data. The present paper work, a Neural Network proposal as  NeuraGen focused which is a  low-resource ANN architecture.  The proposed tool used to classify gender of the speaker from the speech recordings. We have used speech recordings collected from the ELSDSR and limited TIMIT datasets, from which we extracted 8 speech features, which were pre-processed and then fed into NeuraGen to identify the gender. NeuraGen has successfully achieved accuracy of 90.7407% and F1 score of 91.227% in train and 20-fold cross validation dataset.},
  publisher={}
}

